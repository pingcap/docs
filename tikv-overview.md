---
title: TiKV Overview
summary: TiKVは、分散型のトランザクションキー/値データベースであり、ACID準拠のトランザクションAPIを提供します。RocksDBに保存されるRaftコンセンサスアルゴリズムとコンセンサス状態の実装により、複数のレプリカ間のデータの一貫性と高可用性を保証します。TiKVはTiDB分散データベースのstorageレイヤーとして、読み取りおよび書き込みサービスを提供し、アプリケーションから書き込まれたデータを永続化します。TiKVクラスターの統計データも保存されます。TiKVはGoogle Spannerの設計に基づいてマルチラフトグループレプリカメカニズムを実装し、水平方向の拡張性に優れており、100 TBを超えるデータを保存するために簡単に拡張できます。
---

# TiKVの概要 {#tikv-overview}

TiKV は、分散型のトランザクション キー/値データベースであり、 ACID準拠のトランザクション API を提供します。 TiKV は、RocksDB に保存される[Raftコンセンサスアルゴリズム](https://raft.github.io/raft.pdf)およびコンセンサス状態の実装により、複数のレプリカ間のデータの一貫性と高可用性を保証します。 TiKV は、TiDB 分散データベースのstorageレイヤーとして、読み取りおよび書き込みサービスを提供し、アプリケーションから書き込まれたデータを永続化します。 TiDB クラスターの統計データも保存されます。

## アーキテクチャの概要 {#architecture-overview}

TiKV は、Google Spanner の設計に基づいてマルチラフトグループ レプリカ メカニズムを実装します。リージョンは、キーと値のデータ移動の基本単位であり、ストア内のデータ範囲を指します。各リージョンは複数のノードにレプリケートされます。これらの複数のレプリカはRaftグループを形成します。リージョンのレプリカはピアと呼ばれます。通常、リージョンには 3 つのピアが存在します。そのうちの 1 つはリーダーであり、読み取りおよび書き込みサービスを提供します。 PDコンポーネントは、すべてのリージョンのバランスを自動的に調整して、TiKV クラスター内のすべてのノード間で読み取りおよび書き込みスループットのバランスが保たれるようにします。 PD と慎重に設計されたRaftグループを備えた TiKV は、水平方向の拡張性に優れており、100 TB を超えるデータを保存するために簡単に拡張できます。

![TiKV Architecture](/media/tikv-arch.png)

### リージョンとRocksDB {#region-and-rocksdb}

各ストア内には RocksDB データベースがあり、データはローカル ディスクに保存されます。すべてのリージョンデータは、各ストアの同じ RocksDB インスタンスに保存されます。 Raftコンセンサス アルゴリズムに使用されるすべてのログは、各ストアの別の RocksDB インスタンスに保存されます。これは、シーケンシャル I/O のパフォーマンスがランダム I/O よりも優れているためです。 raft ログとリージョンデータを保存するさまざまな RocksDB インスタンスを使用して、TiKV は raft ログと TiKV リージョンのすべてのデータ書き込み操作を 1 つの I/O 操作に結合して、パフォーマンスを向上させます。

### リージョンとRaftのコンセンサスアルゴリズム {#region-and-raft-consensus-algorithm}

リージョンのレプリカ間のデータの一貫性は、 Raftコンセンサス アルゴリズムによって保証されます。リージョンのリーダーのみが書き込みサービスを提供でき、リージョンの大部分のレプリカにデータが書き込まれた場合にのみ書き込み操作が成功します。

TiKV は、クラスター内の各リージョンの適切なサイズを維持しようとします。現在、リージョンサイズはデフォルトで 96 MiB です。このメカニズムは、PDコンポーネントがTiKV クラスター内のノード間でリージョンのバランスをとるのに役立ちます。リージョンのサイズがしきい値 (デフォルトでは 144 MiB) を超えると、TiKV はそれを 2 つ以上のリージョンに分割します。リージョンのサイズがしきい値 (デフォルトでは 20 MiB) より小さい場合、TiKV は 2 つの小さい隣接するリージョンを 1 つのリージョンにマージします。

PD がある TiKV ノードから別の TiKV ノードにレプリカを移動する場合、まずターゲット ノードにLearnerレプリカを追加します。その後、LearnerレプリカのデータがLeaderレプリカのデータとほぼ同じになった後、PD はそれをFollowerレプリカに変更して削除します。ソースノード上のFollowerレプリカ。

Leaderレプリカをあるノードから別のノードに移動する場合も同様のメカニズムがあります。違いは、LearnerレプリカがFollowerレプリカになった後、Followerレプリカが自身をLeaderに選出するための選挙を積極的に提案する「Leader転送」操作があることです。最後に、新しいLeaderはソース ノード内の古いLeaderのレプリカを削除します。

## 分散トランザクション {#distributed-transaction}

TiKV は分散トランザクションをサポートしています。ユーザー (または TiDB) は、同じリージョンに属しているかどうかを気にせずに、複数のキーと値のペアを書き込むことができます。 TiKV は、2 フェーズ コミットを使用してACID制約を実現します。詳細については[TiDB 楽観的トランザクションモデル](/optimistic-transaction.md)を参照してください。

## TiKVコプロセッサー {#tikv-coprocessor}

TiDB は、一部のデータ計算ロジックを TiKVコプロセッサーにプッシュします。 TiKVコプロセッサーは、各リージョンの計算を処理します。 TiKVコプロセッサーに送信される各リクエストには、1 つのリージョンのデータのみが含まれます。
