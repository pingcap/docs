---
title: Operating System Tuning
summary: Learn how to tune the parameters of the operating system.
---

# オペレーティングシステムのチューニング {#operating-system-tuning}

このドキュメントでは、CentOS7の各サブシステムを調整する方法を紹介します。

> **ノート：**
>
> -   CentOS 7オペレーティングシステムのデフォルト構成は、中程度のワークロードで実行されているほとんどのサービスに適しています。特定のサブシステムのパフォーマンスを調整すると、他のサブシステムに悪影響を与える可能性があります。したがって、システムを調整する前に、すべてのユーザーデータと構成情報をバックアップしてください。
> -   実稼働環境に適用する前に、テスト環境ですべての変更を完全にテストします。

## パフォーマンス分析方法 {#performance-analysis-methods}

システムの調整は、システムパフォーマンス分析の結果に基づいている必要があります。このセクションでは、パフォーマンス分析の一般的な方法を示します。

### 60秒で {#in-60-seconds}

[*60,000ミリ秒でのLinuxパフォーマンス分析*](http://www.brendangregg.com/Articles/Netflix_Linux_Perf_Analysis_60s.pdf)は、作者のBrendanGreggとNetflixパフォーマンスエンジニアリングチームによって公開されています。使用するすべてのツールは、Linuxの公式リリースから入手できます。次のリスト項目の出力を分析して、最も一般的なパフォーマンスの問題をトラブルシューティングできます。

-   `uptime`
-   `dmesg | tail`
-   `vmstat 1`
-   `mpstat -P ALL 1`
-   `pidstat 1`
-   `iostat -xz 1`
-   `free -m`
-   `sar -n DEV 1`
-   `sar -n TCP,ETCP 1`
-   `top`

詳細な使用法については、対応する`man`の手順を参照してください。

### パフォーマンス {#perf}

perfは、Linuxカーネルによって提供される重要なパフォーマンス分析ツールであり、ハードウェアレベル（CPU / PMU、パフォーマンス監視ユニット）機能とソフトウェア機能（ソフトウェアカウンター、トレースポイント）をカバーします。詳細な使用法については、 [perfの例](http://www.brendangregg.com/perf.html#Background)を参照してください。

### BCC / bpftrace {#bcc-bpftrace}

CentOS 7.6以降、LinuxカーネルはBerkeley Packet Filter（BPF）をサポートしています。したがって、適切なツールを選択して、 [60秒で](#in-60-seconds)の結果に基づいて詳細な分析を実行できます。 perf / ftraceと比較して、BPFはプログラム可能性とパフォーマンスオーバーヘッドの低減を提供します。 kprobeと比較して、BPFはより高いセキュリティを提供し、実稼働環境により適しています。 BCCツールキットの詳細な使用法については、 [BPFコンパイラコレクション（BCC）](https://github.com/iovisor/bcc/blob/master/README.md)を参照してください。

## 性能調整 {#performance-tuning}

このセクションでは、分類されたカーネルサブシステムに基づくパフォーマンスチューニングを紹介します。

### CPU-周波数スケーリング {#cpu-frequency-scaling}

cpufreqは、CPU周波数を動的に調整するモジュールです。 5つのモードをサポートします。サービスパフォーマンスを確保するには、パフォーマンスモードを選択し、動的調整なしでCPU周波数をサポートされている最高の動作周波数に固定します。この操作のコマンドは`cpupower frequency-set --governor performance`です。

### CPU-割り込みアフィニティ {#cpu-interrupt-affinity}

-   自動バランスは、 `irqbalance`のサービスを通じて実装できます。
-   手動バランス：
    -   割り込みのバランスを取る必要があるデバイスを特定します。 CentOS 7.5以降、システムは、 `be2iscsi`のドライバーとNVMe設定を使用するデバイスなど、特定のデバイスとそのドライバーに最適な割り込みアフィニティを自動的に構成します。このようなデバイスの割り込みアフィニティを手動で設定することはできなくなりました。
    -   その他のデバイスについては、チップのマニュアルをチェックして、これらのデバイスが割り込みの分散をサポートしているかどうかを確認してください。
        -   そうでない場合、これらのデバイスのすべての割り込みは同じCPUにルーティングされ、変更できません。
        -   その場合は、 `smp_affinity`のマスクを計算し、対応する構成ファイルを設定します。詳細については、 [カーネルドキュメント](https://www.kernel.org/doc/Documentation/IRQ-affinity.txt)を参照してください。

### NUMACPUバインディング {#numa-cpu-binding}

Non-Uniform Memory Access（NUMA）ノード間でメモリにアクセスすることをできるだけ回避するために、スレッドのCPUアフィニティを設定することにより、スレッド/プロセスを特定のCPUコアにバインドできます。通常のプログラムでは、CPUバインディングに`numactl`コマンドを使用できます。詳細な使用法については、Linuxのマニュアルページを参照してください。ネットワークインターフェイスカード（NIC）の割り込みについては、 [ネットワークを調整する](#network-tuning)を参照してください。

### メモリ-透明な巨大ページ（THP） {#memory-transparent-huge-page-thp}

データベースのメモリアクセスパターンは連続的ではなくまばらであることが多いため、データベースアプリケーションにTHPを使用することはお勧めし**ません**。高レベルのメモリの断片化が深刻な場合、THPページが割り当てられるときに待ち時間が長くなります。 THPに対して直接圧縮が有効になっている場合、CPU使用率は急上昇します。したがって、THPを無効にすることをお勧めします。

```sh
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
```

### メモリ-仮想メモリパラメータ {#memory-virtual-memory-parameters}

-   `dirty_ratio`パーセント比。ダーティページキャッシュの合計量がシステムメモリ全体のこのパーセンテージに達すると、システムは`pdflush`操作を使用してダーティページキャッシュをディスクに書き込み始めます。デフォルト値の`dirty_ratio`は20％であり、通常は調整する必要はありません。 NVMeデバイスなどの高性能SSDの場合、この値を下げると、メモリ再利用の効率が向上します。
-   `dirty_background_ratio`パーセント比。ダーティページキャッシュの合計量がシステムメモリ全体のこのパーセンテージに達すると、システムはダーティページキャッシュをバックグラウンドでディスクに書き込み始めます。デフォルト値の`dirty_ratio`は10％であり、通常は調整する必要はありません。 NVMeデバイスなどの高性能SSDの場合、低い値を設定すると、メモリ再利用の効率が向上します。

### ストレージとファイルシステム {#storage-and-file-system}

コアI/Oスタックリンクは長く、ファイルシステムレイヤー、ブロックデバイスレイヤー、ドライバーレイヤーが含まれます。

#### I/Oスケジューラ {#i-o-scheduler}

I / Oスケジューラは、ストレージデバイスでI/O操作を実行するタイミングと期間を決定します。 I/Oエレベーターとも呼ばれます。 SSDデバイスの場合、I/Oスケジューリングポリシーをnoopに設定することをお勧めします。

```sh
echo noop > /sys/block/${SSD_DEV_NAME}/queue/scheduler
```

#### フォーマットパラメータ-ブロックサイズ {#formatting-parameters-block-size}

ブロックは、ファイルシステムの作業単位です。ブロックサイズは、1つのブロックに格納できるデータの量を決定し、したがって、毎回書き込まれる、または読み取られるデータの最小量を決定します。

デフォルトのブロックサイズは、ほとんどのシナリオに適しています。ただし、ブロックサイズ（または複数のブロックのサイズ）が、毎回通常読み取られるまたは書き込まれるデータの量と同じかわずかに大きい場合、ファイルシステムのパフォーマンスが向上し、データストレージの効率が高くなります。小さなファイルでもブロック全体が使用されます。ファイルは複数のブロックに分散できますが、これにより実行時のオーバーヘッドが増加します。

`mkfs`コマンドを使用してデバイスをフォーマットする場合は、ファイルシステムオプションの一部としてブロックサイズを指定します。ブロックサイズを指定するパラメータは、ファイルシステムによって異なります。詳細については、 `man mkfs.ext4`の使用など、対応する`mkfs`のマニュアルページを参照してください。

#### <code>mount</code>パラメータ {#code-mount-code-parameters}

`mount`コマンドで`noatime`オプションが有効になっている場合、ファイルの読み取り時にメタデータの更新は無効になります。 `nodiratime`の動作が有効になっている場合、ディレクトリの読み取り時にメタデータの更新は無効になります。

### ネットワークチューニング {#network-tuning}

ネットワークサブシステムは、機密性の高い接続を持つ多くの異なる部分で構成されています。 CentOS 7ネットワークサブシステムは、ほとんどのワークロードに最高のパフォーマンスを提供し、これらのワークロードのパフォーマンスを自動的に最適化するように設計されています。したがって、通常、ネットワークパフォーマンスを手動で調整する必要はありません。

ネットワークの問題は通常、ハードウェアまたは関連デバイスの問題によって引き起こされます。したがって、プロトコルスタックを調整する前に、ハードウェアの問題を除外してください。

ネットワークスタックは主に自己最適化されていますが、ネットワークパケット処理の次の側面がボトルネックになり、パフォーマンスに影響を与える可能性があります。

-   NICハードウェアキャッシュ：ハードウェアレベルでパケット損失を正しく監視するには、 `ethtool -S ${NIC_DEV_NAME}`コマンドを使用して`drops`フィールドを監視します。パケット損失が発生した場合、ハード/ソフト割り込みの処理速度がNICの受信速度に追いつかない可能性があります。受信したバッファサイズが上限よりも小さい場合は、パケット損失を回避するためにRXバッファを増やすこともできます。クエリコマンドは`ethtool -g ${NIC_DEV_NAME}` 、変更コマンドは`ethtool -G ${NIC_DEV_NAME}`です。

-   ハードウェア割り込み：NICが受信側スケーリング（RSS、マルチNIC受信とも呼ばれる）機能をサポートしている場合は、 `/proc/interrupts`のNIC割り込みを確認します。割り込みが不均一な場合は、 [CPU-周波数スケーリング](#cpufrequency-scaling) 、および[CPU-割り込みアフィニティ](#cpuinterrupt-affinity)を参照して[NUMACPUバインディング](#numa-cpu-binding) 。 NICがRSSをサポートしていない場合、またはRSSの数が物理CPUコアの数よりもはるかに少ない場合は、Receive Packet Steering（RPS、RSSのソフトウェア実装と見なすことができます）およびRPS拡張Receive Flow Steering（ RFS）。詳細な構成については、 [カーネルドキュメント](https://www.kernel.org/doc/Documentation/networking/scaling.txt)を参照してください。

-   ソフトウェア割り込み： `/proc/net/softnet_stat`の監視を観察します。 3番目の列を除く他の列の値が増加している場合は、 `net.core.netdev_budget`または`net.core.dev_weight`の値を`softirq`に適切に調整して、CPU時間を増やします。さらに、CPU使用率をチェックして、CPUを頻繁に使用しているタスクと、それらを最適化できるかどうかを判断する必要もあります。

-   アプリケーションソケットの受信キュー： `ss -nmp`の`Resv-q`列を監視します。キューがいっぱいの場合は、アプリケーションソケットキャッシュのサイズを増やすか、自動キャッシュ調整方法を使用することを検討してください。さらに、アプリケーション層のアーキテクチャを最適化し、ソケットの読み取り間隔を短縮できるかどうかを検討してください。

-   イーサネットフロー制御：NICとスイッチがフロー制御機能をサポートしている場合、この機能を使用して、カーネルがNICキュー内のデータを処理する時間を確保し、NICバッファオーバーフローの問題を回避できます。

-   割り込みの合体：ハードウェア割り込みが多すぎるとシステムパフォーマンスが低下し、ハードウェア割り込みが遅すぎるとパケット損失が発生します。新しいNICは、割り込み合体機能をサポートし、ドライバーがハードウェア割り込みの数を自動的に調整できるようにします。 `ethtool -c ${NIC_DEV_NAME}`を実行してチェックし、 `ethtool -C ${NIC_DEV_NAME}`を実行してこの機能を有効にすることができます。アダプティブモードにより、NICは割り込み合体を自動的に調整できます。このモードでは、ドライバーはトラフィックモードとカーネル受信モードをチェックし、パケット損失を防ぐために合体設定をリアルタイムで評価します。異なるブランドのNICには、異なる機能とデフォルト構成があります。詳細については、NICのマニュアルを参照してください。

-   アダプタキュー：プロトコルスタックを処理する前に、カーネルはこのキューを使用してNICが受信したデータをバッファリングし、各CPUには独自のバックログキューがあります。このキューにキャッシュできるパケットの最大数は`netdev_max_backlog`です。 `/proc/net/softnet_stat`の2番目の列を観察します。行の2番目の列が増え続ける場合は、CPU [row-1]キューがいっぱいで、データパケットが失われていることを意味します。この問題を解決するには、引き続き`net.core.netdev_max_backlog`の値を2倍にします。

-   送信キュー：送信キューの長さの値によって、送信前にキューに入れることができるパケットの数が決まります。デフォルト値は`1000`で、10Gbpsには十分です。しかし、 `ip -s link`の出力からTXエラーの値を観察した場合は、それを2倍にすることができます： `ip link set dev ${NIC_DEV_NAME} txqueuelen 2000` 。

-   Driver：NICドライバーは通常、チューニングパラメーターを提供します。デバイスのハードウェアマニュアルとそのドライバのドキュメントを参照してください。
