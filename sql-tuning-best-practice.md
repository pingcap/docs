---
title: A Practical Guide For SQL Tuning
summary: Learn how to do SQL tuning in TiDB
---

# Document purpose and objectives

This guide is designed for TiDB SQL tuning beginners, with emphasis on:

- Low Entry Barrier: Introducing tuning concepts and methods progressively
- Practice-Oriented: Each optimization tip comes with specific steps and examples
- Quick Start: Prioritizing the most common and effective optimization methods
- Gentle Learning Curve: Focusing on practicality over theoretical complexity
- Scenario-Based: Incorporating real business cases to demonstrate optimization effects

# Introduction to SQL tuning

SQL tuning is a critical aspect of optimizing database system performance. It involves a systematic approach to improve the efficiency of SQL queries. The process typically consists of three key steps:

1. Identify high-impact SQL statements:
   - Review SQL execution history to find statements that consume a large portion of system resources or contribute significantly to the application workload.
   - Use monitoring tools and performance metrics to pinpoint these resource-intensive queries.

2. Analyze execution plans:
   - Examine the execution plans generated by the query optimizer for the identified statements.
   - Verify that these plans are reasonably efficient and utilize appropriate indexes and join methods.

3. Implement optimizations:
   - Apply corrective actions to improve poorly performing SQL statements.
   - This may include rewriting queries, adding or modifying indexes, updating statistics, or adjusting database parameters.

These steps are iteratively repeated until:

- The system performance meets the desired targets
- No further improvements can be made to the remaining statements.

It's important to note that SQL tuning is an ongoing process. As your data volumes grow and query patterns evolve, you should:

- Regularly monitor query performance
- Re-evaluate your optimization strategies
- Adapt your approach to address new performance challenges

By consistently applying these practices, you can ensure that your database maintains optimal performance over time.

# Goals for tuning

The primary objectives of SQL tuning are to:

1. Reduce response time for end users
2. Minimize resource consumption for processing workloads

These goals can be achieved through various strategies:

## Optimize query execution

SQL tuning often involves finding more efficient ways to process the same workload without changing the query's functionality. This can be done by:

1. Improving execution plans:
   - Analyze and modify query structures to enable more efficient processing
   - Utilize appropriate indexes to reduce data access and processing time
   - Enable TiFlash for analytical queries on large volumes of data and Leverage MPP (Massively Parallel Processing) engine for complex aggregations and joins

2. Enhancing data access methods:
   - Use covering indexes to satisfy queries directly from the index, avoiding table access
   - Implement partitioning strategies to limit data scans to relevant partitions

Examples:

- Creating an index for frequently queried columns can significantly reduce resource usage, especially for queries that access a small percentage of table data.
- Utilizing index-only scans for queries that return a limited number of sorted results can avoid full table scans and sorting operations.

## Balance workload distribution

In a distributed architecture like TiDB, it's essential to maintain a balanced workload across multiple TiKV nodes. This is crucial for achieving optimal performance. For guidance on identifying and resolving read and write hotspots, please refer to the [troubleshoot hot spot issues page](/troubleshoot-hot-spot-issues.md#optimization-of-small-table-hotspots). By implementing these strategies, you can ensure that your TiDB cluster efficiently utilizes all available resources and avoids bottlenecks caused by uneven workload distribution or serialization on individual TiKV nodes.

# Identifying high-load SQL

The most efficient way to identify Resource-Intensive SQL is using TiDB Dashboard. There are other tools like views and logs available as well.

## Monitoring SQL statements by using TiDB Dashboard

### SQL Statements panel 

In [TiDB Dashboard](dashboard/dashboard-overview.md), navigate to SQL Statements panel, which helps us identify the following:

1. The SQL statement with the highest total latency is the one that takes the longest time to execute out of multiple executions of the same SQL statement. 
2. It can also display the number of times each SQL statement has been executed cumulatively, allowing us to find the SQL statement with the highest execution frequency.
3. Clicking on each SQL statement allows us to delve deeper into the `EXPLAIN ANALYZE` results.

SQL statements are normalized as templates, where literals and bind variables are replaced by '?'. This normalization and sorting process allows you to quickly pinpoint the most resource-intensive queries that may require optimization.

![sql-statements-default](/media/sql-tuning/sql-statements-default.png)

### Slow Queries panel default display

On Slow Queries panel, we can find:

1. The slowest SQL queries.
2. The SQL query that reads the most data from TiKV.
3. The `EXPLAIN ANALYZE` output from drilling down the SQL statement by clicking it.
4. Please note that on the Slow Queries panel, we cannot get the frequency of the SQL statement execution. Once the execution elapsed time exceeds [`tidb_slow_log_threshold`](/tidb-configuration-file.md#tidb_slow_log_threshold) for single instance, the query is then listed on the Slow Queries panel.

![slow-query-default](/media/sql-tuning/slow-query-default.png)

## Other tools for identifying Top SQL

In addition to TiDB Dashboard, there are several other tools available to identify resource-intensive SQL queries:

Each tool offers unique insights and can be valuable for different analysis scenarios. Using a combination of these tools allows for comprehensive SQL performance monitoring and optimization.

- [Top SQL feature](/dashboard/top-sql.md)
- Logs: [slow query log](/identify-slow-queries.md) and [expensive queries in TiDB log](/identify-expensive-queries.md)
- Views: [cluster_statements_summary view](/statement-summary-tables.md##the-cluster-tables-for-statement-summary) and [cluster_processlist view](/information-schema/information-schema-processlist.md#cluster_processlist)

## Gathering data on the SQL identified

For the top SQL statements identified, you can use [PLAN REPLAYER](/sql-plan-replayer.md) to capture and save the on-site information of a TiDB cluster. This tool allows you to recreate the execution environment for further analysis. The syntax for exporting SQL information is as follows:

```sql
PLAN REPLAYER DUMP EXPLAIN [ANALYZE] [WITH STATS AS OF TIMESTAMP expression] sql-statement;
```

Use `EXPLAIN ANALYZE` whenever possible, as it captures actual execution information in addition to the execution plan. This provides more accurate insights into query performance.

# SQL tuning guide

This guide focuses on providing actionable advice for beginners looking to optimize their SQL queries in TiDB. By following these best practices, you can ensure better query performance and SQL Tuning. We'll cover below topic:

- Query Processing Workflow
- Optimizer Fundamentals
- Statistics Management
- How TiDB Builds A Execution Plan
- Understand Execution Plan
- Index Strategy in TiDB
    - Composite Index Strategy Guidelines
    - The Cost of Indexing
    - SQL Tuning with a Covered Index
    - SQL Tuning with a Composite Index Involing Sorting
    - SQL Tuning with a Composite Index for Efficient Filtering and Sorting
- When to Use TiFlash: A Simple Guide

## Query processing workflow

The client sends a SQL statement to the protocol layer of TiDB server. The protocol layer is responsible for handling the connection between TiDB server and the client, receiving SQL statement from the client, and returning data to the client.

To the right of the protocol layer is the optimizer of TiDB server, which is responsible for processing SQL statements. The process is as follows:

1. SQL statement arrives at the SQL optimizer through the protocol layer and is first parsed into an Abstract Syntax Tree (AST).
2. Pre-Process is primarily for [Point Get](/explain-indexes#point_get-and-batch_point_get), a simple, one table lookup through a Primary or Unique Key like `SELECT * FROM t WHERE pk_col = 1` or `SELECT * FROM t WHERE uk_col IN (1,2,3)`. If it is a `Point Get`, the following-up optimization processes can be skipped, the next step jumps to the SQL Executor.
3. After confirming that it is not a `Point Get`, the AST goes to the Logical Transformation, which rewrites the SQL logically based on certain rules.
4. The AST that has gone through Logical Transformation will undergo Cost-Based Optimization.
5. During Cost-Based Optimization, the optimizer considers statistics to determine how to select specific operators and finally generates a physical execution plan.
6. The generated physical execution plan is sent to the SQL Executor of the TiDB node for execution.
7. Unlike traditional single node databases, TiDB will push down operators/coprocessors to TiKV and/or TiFlash nodes containing the data, to process the parts of the execution plan where the data is stored, to more efficiently utilize the distributed nature, use resources in parallel, and send less data over the network. The executor in the TiDB node will assemble the final result and send it back to the client.

![workflow](/media/sql-tuning/workflow-tiflash.png)

## Optimizer fundamentals

TiDB uses a cost-based optimizer (CBO) to determine the most efficient execution plan for a SQL statement. This optimizer evaluates different execution strategies and chooses the one with the lowest estimated cost. The cost is influenced by factors such as:

- SQL
- Schema Design
- Statistics
    - Table
    - Index
    - Column

Based on the input, The cost model will produce the execution plan, which includes the details how the system execute the sql, including 

- Access Method
- Join Method
- Join Order

The optimizer is as good as the information it receives. Therefore, ensuring up-to-date statistics and well-designed indexes is critical.

## Statistics management

Statistics are essential to the TiDB optimizer. TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement.

Statistics is generally divided into two levels: table level and index/column level. 

- For table level statistics, it includes the total number of rows in the table and the number of rows that have been modified since the last collection of statistics. 
- The index/column level statistics information is more abundant, including histograms, Count-Min Sketch, Top-N (values or indexes with the highest occurrences), distribution and quantity of different values, and the number of null values, and so on.

To ensure the statistics are healthy and representative, you can use the following commands:

1. [`SHOW STATS_META`](sql-statements/sql-statement-show-stats-meta.md): This command provides metadata about table statistics.
2. [`SHOW STATS_HEALTHY`](sql-statements/sql-statement-show-stats-healthy.md): This command shows the health status of table statistics.

For example, you can use:

```sql
SHOW STATS_META WHERE table_name='T2'\G;
```

```
*************************** 1. row ***************************
          Db_name: test
       Table_name: T2
   Partition_name:
      Update_time: 2023-05-11 02:16:50
     Modify_count: 10000
        Row_count: 20000
1 row in set (0.03 sec)
```

```sql
SHOW STATS_HEALTHY WHERE table_name='T2'\G;
```

```
*************************** 1. row ***************************
       Db_name: test
    Table_name: T2
Partition_name:
       Healthy: 50
1 row in set (0.00 sec)
```

In TiDB database, there are two ways to collect statistics: automatic collection and manual collection. In most case, the auto collection job works fine. Automatic collection is triggered when certain conditions are met for a table, and TiDB will automatically collect statistics. We commonly use three triggering conditions, which are: ratio, start_time and end_time.

- [`tidb_auto_analyze_ratio`](/system-variables.md#tidb_auto_analyze_ratio): The healthiness trigger
- [`tidb_auto_analyze_start_time`](/system-variables.md#tidb_auto_analyze_start_time) and [`tidb_auto_analyze_end_time`](/system-variables.md#tidb_auto_analyze_end_time): The allowed job window

```sql
SHOW VARIABLES LIKE 'tidb\_auto\_analyze%';
```

```
+-----------------------------------------+-------------+
| Variable_name                           | Value       |
+-----------------------------------------+-------------+
| tidb_auto_analyze_ratio                 | 0.5         |
| tidb_auto_analyze_start_time            | 00:00 +0000 |
| tidb_auto_analyze_end_time              | 23:59 +0000 |
+-----------------------------------------+-------------+
```

There are cases where automatic collection doesn't meet your needs. The analyze windown by default is `00:00` to `23:59`, which means the analyze job can be triggered any time during the day. If you want to trigger the analyze job only during certain hours, you can set the start time and end time, to avoid performance impact for the online business.

You can manually collect statistics using the `ANALYZE TABLE table_name` statement. This allows you to adjust the default settings, such as the sample rate, number of top-N values, or only gathering statistics for specific columns only. 

It's important to note that after manual collection, subsequent automatic gathering jobs will inherit the new settings. This means that any customizations you've made during manual collection will be carried forward in future automatic analyses.

Another common scenario is locking table statistics. This is useful when:

1. The statistics on the table are already representative of the data.
2. The table is very large and statistics collection is time-consuming.
3. You want to maintain statistics only during specific time windows.

To lock the statistics for a table, you can use the following statement [`LOCK STATS table_name`](/sql-statements/sql-statement-lock-stats.md).

for more detail about statistics, please refer to [statistics](/statistics.md).

## How TiDB builds a execution plan

An SQL statement undergoes optimization primarily in the optimizer through three stages:

- Pre-Processing
- Logical Transformation
- Cost-based Optimization

### Pre-processing

The main actions in the pre-processing stage it to determine if the SQL statement can be executed by using [`Point_Get`](/explain-indexes#point_get-and-batch_point_get) or [`Batch_Point_Get`](/explain-indexes#point_get-and-batch_point_get). `Point_Get` or `Batch_Point_Get` means using a primary or unique key, to directly read from TiKV, by exact key lookup. If a plan is identified as `Point_Get` or `Batch_Point_Get`, optimizer will skip the logical transformation and cost-based optimization, since the exact key read will be the best way to access the row.

Here is the query statement:

```sql
explain SELECT id, name FROM emp WHERE id = 901;
```

```
+-------------+---------+------+---------------+---------------+
| id          | estRows | task | access object | operator info |
+-------------+---------+------+---------------+---------------+
| Point_Get_1 | 1.00    | root | table:emp     | handle:901    |
+-------------+---------+------+---------------+---------------+
```

### Logical transformation

The purpose of logical Transformation is to optimize the execution of statements based on the characteristics of SELECT list, WHERE predicates, and other predicates in SQL queries. It generates a logical execution plan to annotate and rewrite the query. This logical plan is then passed to the Cost-Based Optimization. The optimization rules are such as column pruning, partition pruning, join reorder etc. Since this step is rule-based and automated by the query optimizer, in most cases it usually does not require manual adjustments.

More Detail for Logical Transformation: [SQL Logical Optimization](/sql-logical-optimization.md).

### Cost-Based optimization

TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement, and associates a cost with each plan step. The Cost-Based Optimization estimates the cost of each available plan choice, including index accesses and join methods, and produces a cost for each available plan. The optimizer then picks the execution plan with the lowest overall cost.

The following figure illustrates the various data access paths and row set operations that cost-based optimization can consider when developing the optimal execution plan. For data access paths, the optimizer determines the most efficient method to retrieve data, whether through an index scan or a table scan, and whether to retrieve the data from row-based TiKV or columnar-based TiFlash storage.

Also, the optimizer need to evaluate operations that manipulate row sets, such as aggregation, join, and sorting. For instance, the aggregation operator may utilize either `HashAgg` or `StreamAgg`, while the join method can select from `HashJoin`, `MergeJoin`, or `IndexJoin`. 

Furthermore, expression and operator push-down to the physical storage engines is part of the physical optimization phase. The physical plan is  distributed to different components based on the underlying storage engines:

- Root Task runs at the TiDB Server
- Cop (Coprocessor) Task runs at TiKV
- MPP Task runs at TiFlash

This distribution of the physical plan allows the different components to collaborate and execute the query efficiently.

![cost-based-optimization](/media/sql-tuning/cost-based-optimization.png)

More Detail for Cost-Based Optimization: [SQL Physical Optimization](/sql-physical-optimization.md).

## Understanding execution plans

The execution plan represents the steps TiDB will follow to execute a SQL query. In this section, we will learn how to display and read the execution plan.

### Generating and displaying execution plans

Beside access the execution plan information through TiDB Dashboard, TiDB provides a `EXPLAIN` statement to display the execution plan for a SQL query. Here's an example of using `EXPLAIN`:

- id: Operator name and the step unique identifier
- estRows: Estimated number of rows from the particular step
- task: Indicates the layer where the operator is executed. For instance, `root` indicates execution at the TiDB Server, whereas `cop[tikv]` indicates execution at TiKV, and `mpp[tiflash]` indicates execution at TiFlash. 
- access object: The object where the row sources are located
- operator info: Extended information about the operator regarding the step

```sql
EXPLAIN SELECT COUNT(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';
```

```
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| id                       | estRows     | task         | access object     | operator info                                                                                      |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| StreamAgg_20             | 1.00        | root         |                   | funcs:count(Column#13)->Column#11                                                                  |
| └─TableReader_21         | 1.00        | root         |                   | data:StreamAgg_9                                                                                   |
|   └─StreamAgg_9          | 1.00        | cop[tikv]    |                   | funcs:count(1)->Column#13                                                                          |
|     └─Selection_19       | 250.00      | cop[tikv]    |                   | ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000) |
|       └─TableFullScan_18 | 10000.00    | cop[tikv]    | table:trips       | keep order:false, stats:pseudo                                                                     |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
5 rows in set (0.00 sec)
```

Additional Information in [EXPLAIN ANALYZE](sql-statements/sql-statement-explain-analyze.md) Output. Different from `EXPLAIN`, `EXPLAIN ANALYZE` executes the corresponding SQL statement, records its runtime information, and returns the information together with the execution plan. There runtime information is crucial for debugging query execution.

Description

- actRows: Number of rows output by the operator.
- execution info: Detailed execution information of the operator. `time` usually represents the total wall time, including the total execution time of all sub-operators. If the operator is called many times by the parent operator then the time refers to the accumulated time.
- memory: Memory used by the operator.
- disk: Disk space used by the operator.

Note: Some attributes and explain table columns are omitted for improved formatting

Here is the query statement:

```sql
EXPLAIN ANALYZE
SELECT SUM(pm.m_count) / COUNT(*) 
FROM (
    SELECT COUNT(m.name) m_count
    FROM universe.moons m
    RIGHT JOIN (
        SELECT p.id, p.name
        FROM universe.planet_categories c
        JOIN universe.planets p
            ON c.id = p.category_id 
            AND c.name = 'Jovian'
    ) pc ON m.planet_id = pc.id
    GROUP BY pc.name
) pm;
```

```
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| id                                      |.| actRows | task      | access object             | execution info                                                 |.| memory    | disk    |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| Projection_14                           |.| 1       | root      |                           | time:1.39ms, loops:2, RU:1.561975, Concurrency:OFF             |.| 9.64 KB   | N/A     |
| └─StreamAgg_16                          |.| 1       | root      |                           | time:1.39ms, loops:2                                           |.| 1.46 KB   | N/A     |
|   └─Projection_40                       |.| 4       | root      |                           | time:1.38ms, loops:4, Concurrency:OFF                          |.| 8.24 KB   | N/A     |
|     └─HashAgg_17                        |.| 4       | root      |                           | time:1.36ms, loops:4, partial_worker:{...}, final_worker:{...} |.| 82.1 KB   | N/A     |
|       └─HashJoin_19                     |.| 25      | root      |                           | time:1.29ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 2.25 KB   | 0 Bytes |
|         ├─HashJoin_35(Build)            |.| 4       | root      |                           | time:1.08ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 25.7 KB   | 0 Bytes |
|         │ ├─IndexReader_39(Build)       |.| 1       | root      |                           | time:888.5µs, loops:2, cop_task: {...}                         |.| 286 Bytes | N/A     |
|         │ │ └─IndexRangeScan_38         |.| 1       | cop[tikv] | table:c, index:name(name) | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         │ └─TableReader_37(Probe)       |.| 10      | root      |                           | time:543.7µs, loops:2, cop_task: {...}                         |.| 577 Bytes | N/A     |
|         │   └─TableFullScan_36          |.| 10      | cop[tikv] | table:p                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         └─TableReader_22(Probe)         |.| 28      | root      |                           | time:671.7µs, loops:2, cop_task: {...}                         |.| 876 Bytes | N/A     |
|           └─TableFullScan_21            |.| 28      | cop[tikv] | table:m                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
```

### Reading execution plans: first child first

To understand why SQL queries run slowly, it's very important to know how to read Execution Plans. The main rule for reading an execution plan is "first child first – recursive descent". Each operator in the plan produces a set of rows, and the execution order determines how these rows flow through the plan tree. 

The "first child first" rule means that before an operator can produce its output rows, it must first get the rows from all its child operators. For example, a join operator needs rows from both its child operators before it can perform the join operation. The "recursive descent" rule means we analyze the plan from top to bottom, but the actual data flows from bottom to top, as each operator depends on its children's output.

There are two important details to add to this:

1. Parent-Child Interaction: Although a parent operator calls its child operators in sequence, it may cycle through them multiple times. For example, in an index lookup or nested loop join, the parent fetches a batch of rows from the first child, then (zero or more) rows from the second child, repeating this process until it consumes the entire result set from the first child.

2. Blocking vs. Non-blocking Operators: Operators can be either blocking or non-blocking. Blocking operators, such as `TopN` and `HashAgg`, must create their entire result set before passing anything to their parent. Non-blocking operators, like `IndexLookup` and `IndexJoin`, create and pass their row source piece by piece on demand.

Let's apply the "first child first – recursive descent" rule to the first plan. When reading an execution plan, you should start from the top and work your down bottom. In the below example, the final child operator is `TableFullScan_18` (or the leaf node of the plan tree). In this case the access operator is full table scan. The rows produced by the tables scans will be consumed by the `Selection_19` operator. The `Selection_19` operator is to filter the data by `ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000)`. Next the group-by operator `StreamAgg_9` is to implemented the aggregation `count(*)`. Be noted that the 3 operators `TableFullScan_18`, `Selection_19`, `StreamAgg_9` are pushdown to TiKV, which is marked as `cop[tikv]`, so that early filter and aggregation can be done in TiKV, to minize the data transfer between TiKV and TiDB. Finally the `TableReader_21` is to read the data from the `StreamAgg_9` operator, then finally the `StreamAgg_20` is to implemented the aggregation `count(*)`.

```sql
EXPLAIN SELECT COUNT(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';
```

```
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| id                       | estRows     | task         | access object     | operator info                                                                                      |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| StreamAgg_20             | 1.00        | root         |                   | funcs:count(Column#13)->Column#11                                                                  |
| └─TableReader_21         | 1.00        | root         |                   | data:StreamAgg_9                                                                                   |
|   └─StreamAgg_9          | 1.00        | cop[tikv]    |                   | funcs:count(1)->Column#13                                                                          |
|     └─Selection_19       | 250.00      | cop[tikv]    |                   | ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000) |
|       └─TableFullScan_18 | 10000.00    | cop[tikv]    | table:trips       | keep order:false, stats:pseudo                                                                     |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
5 rows in set (0.00 sec)
```

Let's apply the "first child first – recursive descent" rule to the second plan. In the below example begin from the top to bottom, by looking at the `IndexRangeScan_47` (the first leaf of the plan tree). For the table `stars`, the optimizer only needs to select the column `name` and `id`, the two columns can be met by the index `name(name)`. So for the table `star`, the root reader is `IndexReader_48`, rather than a `TableReader`. The join method between `stars` and `planets` is a hash join, which is marked as `HashJoin_44`. The data access method on `planets` is a `TableFullScan_45`. After the join, the `TopN_26` and `TopN_19` are to implement the `ORDER BY` and `LIMIT` clauses correspondingly. The final operator `Projection_16` is to implement the column projection for `t5.name`.

Here is the query statement:

```sql
EXPLAIN 
SELECT t5.name 
FROM (
    SELECT p.name, p.gravity, p.distance_from_sun 
    FROM universe.planets p 
    JOIN universe.stars s
        ON s.id = p.sun_id 
        AND s.name = 'Sun'
    ORDER BY p.distance_from_sun ASC 
    LIMIT 5
) t5
ORDER BY t5.gravity DESC 
LIMIT 3;
```

```
+-----------------------------------+----------+-----------+---------------------------+
| id                                | estRows  | task      | access object             |
+-----------------------------------+----------+-----------+---------------------------+
| Projection_16                     | 3.00     | root      |                           |
| └─TopN_19                         | 3.00     | root      |                           |
|   └─TopN_26                       | 5.00     | root      |                           |
|     └─HashJoin_44                 | 5.00     | root      |                           |
|       ├─IndexReader_48(Build)     | 1.00     | root      |                           |
|       │ └─IndexRangeScan_47       | 1.00     | cop[tikv] | table:s, index:name(name) |
|       └─TableReader_46(Probe)     | 10.00    | root      |                           |
|         └─TableFullScan_45        | 10.00    | cop[tikv] | table:p                   |
+-----------------------------------+----------+-----------+---------------------------+
```

Here is a figure illustrating the plan tree for the second execution plan:

![execution-plan-traverse](/media/sql-tuning/execution-plan-traverse.png)

The traversal of the execution plan follows a top-to-bottom, first-child-first approach. This traversal pattern corresponds to a postorder (Left, Right, Root) traversal of the plan tree.

To read this plan:

1. Start at the top with `Projection_16`
2. Move to its child, `TopN_19`
3. Continue to `TopN_26`
4. Proceed to `HashJoin_44`
5. For `HashJoin_44`, first process its left (Build) child:
   - `IndexReader_48`
   - `IndexRangeScan_47`
6. Then process its right (Probe) child:
   - `TableReader_46`
   - `TableFullScan_45`

This traversal ensures that each operator's inputs are processed before the operator itself, allowing for efficient execution of the query plan.

### Identifying and understanding bottlenecks in execution plans

When reading the execution plan, it's crucial to compare the `actRows` (actual rows) with the `estRows` (estimated rows) to assess the accuracy of the optimizer's estimations. A significant discrepancy between these values may indicate that the optimizer's statistics are outdated or inaccurate, potentially leading to suboptimal query plans.

To identify the bottleneck in a poorly performing query:

1. Scan the `execution info` section from top to bottom, looking for operators that consume a significant amount of time.
2. For the first child operator with significant time consumption, analyze the following:
   - `actRows`: Compare with `estRows` to check for estimation accuracy.
   - Detailed measurements in `execution info`: Look for high values in time, or other metrics.
   - `memory` and `disk` usage: High values may indicate suboptimal plan or resource constraints.
3. Correlate these factors to determine the root cause of the performance issue. For example, if you see a `TableFullScan` operation with a high `actRows` count and significant time in `execution info`, it might suggest the need for an index. Alternatively, if a `HashJoin` operation shows high memory usage and time, you might need to optimize the join order or consider alternative join methods.

In the execution plan below, the query ran for 5 minutes and 51 seconds before being canceled. Let's analyze the key issues:

1. Severe underestimation: The first leaf node `IndexReader_76` reads data from the index `index_orders_on_adjustment_id(adjustment_id)`. The actual number of rows (`actRows`) is 256,811,189, which is drastically higher than the estimated 1 row (`estRows`).
2. Memory overflow: Due to this underestimation, the hash join operator `HashJoin_69` attempts to build a hash table with far more data than anticipated. This results in excessive memory usage (22.6GB) and disk usage (7.65GB).
3. Query termination: The `actRows` is 0 for `HashJoin_69` and the operators above, indicating either there is no rows matched or the query is terminated due to resource constraints. In this case, the hash join consumed too much memory, causing the query to be terminated by memory control mechanisms.
4. Incorrect join order: The root cause of this inefficient plan is the severe underestimation of `estRows` for `IndexRangeScan_75`, which led to an incorrect join order decision by the optimizer.

To address these issues, need to ensure that table statistics are up-to-date, especially for the `orders` table and the `index_orders_on_adjustment_id` index.

```
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...----------------------+
| id                                 | estRows   | estCost      | actRows   | task      | access object                                                                          | execution info ...| memory   | disk     |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...----------------------+
| TopN_19                            | 1.01      | 461374372.63 | 0         | root      |                                                                                        | time:5m51.1s, l...| 0 Bytes  | 0 Bytes  |
| └─IndexJoin_32                     | 1.01      | 460915067.45 | 0         | root      |                                                                                        | time:5m51.1s, l...| 0 Bytes  | N/A      |
|   ├─HashJoin_69(Build)             | 1.01      | 460913065.41 | 0         | root      |                                                                                        | time:5m51.1s, l...| 21.6 GB  | 7.65 GB  |
|   │ ├─IndexReader_76(Build)        | 1.00      | 18.80        | 256805045 | root      |                                                                                        | time:1m4.1s, lo...| 12.4 MB  | N/A      |
|   │ │ └─IndexRangeScan_75          | 1.00      | 186.74       | 256811189 | cop[tikv] | table:orders, index:index_orders_on_adjustment_id(adjustment_id)                       | tikv_task:{proc...| N/A      | N/A      |
|   │ └─Projection_74(Probe)         | 30652.93  | 460299612.60 | 1024      | root      |                                                                                        | time:1.08s, loo...| 413.4 KB | N/A      |
|   │   └─IndexLookUp_73             | 30652.93  | 460287375.95 | 6144      | root      | partition:all                                                                          | time:1.08s, loo...| 107.8 MB | N/A      |
|   │     ├─IndexRangeScan_70(Build) | 234759.64 | 53362737.50  | 390699    | cop[tikv] | table:rates, index:index_rates_on_label_id(label_id)                                   | time:29.6ms, lo...| N/A      | N/A      |
|   │     └─Selection_72(Probe)      | 30652.93  | 110373973.91 | 187070    | cop[tikv] |                                                                                        | time:36.8s, loo...| N/A      | N/A      |
|   │       └─TableRowIDScan_71      | 234759.64 | 86944962.10  | 390699    | cop[tikv] | table:rates                                                                            | tikv_task:{proc...| N/A      | N/A      |
|   └─TableReader_28(Probe)          | 0.00      | 43.64        | 0         | root      |                                                                                        |                ...| N/A      | N/A      |
|     └─Selection_27                 | 0.00      | 653.96       | 0         | cop[tikv] |                                                                                        |                ...| N/A      | N/A      |
|       └─TableRangeScan_26          | 1.01      | 454.36       | 0         | cop[tikv] | table:labels                                                                           |                ...| N/A      | N/A      |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...----------------------+
```

Here is the expected execution plan after fixing the incorrect estimation on the `orders` table. The query now takes 1.96 seconds to run, which is a significant improvement from the previous 5 minutes and 51 seconds:

1. Accurate estimation: The `estRows` values are now much closer to the `actRows`, indicating that the statistics have been updated and are more accurate.
2. Efficient join order: The query now starts with a `TableReader` on the `labels` table, followed by an `IndexJoin` with the `rates` table, and finally another `IndexJoin` with the `orders` table. This join order is more efficient given the actual data distribution.
3. No memory overflow: Unlike the previous plan, there are no signs of excessive memory or disk usage, indicating that the query executes within expected resource limits.

This optimized plan demonstrates the importance of accurate statistics and proper join order in query performance. The dramatic reduction in execution time (from 351 seconds to 1.96 seconds) highlights the impact of addressing estimation errors. 

```
+---------------------------------------+----------+---------+-----------+----------------------------------------------------------------------------------------+---------------...+----------+------+
| id                                    | estRows  | actRows | task      | access object                                                                          | execution info...| memory   | disk |
+---------------------------------------+----------+---------+-----------+----------------------------------------------------------------------------------------+---------------...+----------+------+
| Limit_24                              | 1000.00  | 1000    | root      |                                                                                        | time:1.96s, lo...| N/A      | N/A  |
| └─IndexJoin_88                        | 1000.00  | 1000    | root      |                                                                                        | time:1.96s, lo...| 1.32 MB  | N/A  |
|   ├─IndexJoin_99(Build)               | 1000.00  | 2458    | root      |                                                                                        | time:1.96s, lo...| 77.7 MB  | N/A  |
|   │ ├─TableReader_109(Build)          | 6505.62  | 158728  | root      |                                                                                        | time:1.26s, lo...| 297.0 MB | N/A  |
|   │ │ └─Selection_108                 | 6505.62  | 171583  | cop[tikv] |                                                                                        | tikv_task:{pro...| N/A      | N/A  |
|   │ │   └─TableRangeScan_107          | 80396.43 | 179616  | cop[tikv] | table:labels                                                                           | tikv_task:{pro...| N/A      | N/A  |
|   │ └─Projection_98(Probe)            | 1000.00  | 2458    | root      |                                                                                        | time:2.13s, lo...| 59.2 KB  | N/A  |
|   │   └─IndexLookUp_97                | 1000.00  | 2458    | root      | partition:all                                                                          | time:2.13s, lo...| 1.20 MB  | N/A  |
|   │     ├─Selection_95(Build)         | 6517.14  | 6481    | cop[tikv] |                                                                                        | time:798.6ms, ...| N/A      | N/A  |
|   │     │ └─IndexRangeScan_93         | 6517.14  | 6481    | cop[tikv] | table:rates, index:index_rates_on_label_id(label_id)                                   | tikv_task:{pro...| N/A      | N/A  |
|   │     └─Selection_96(Probe)         | 1000.00  | 2458    | cop[tikv] |                                                                                        | time:444.4ms, ...| N/A      | N/A  |
|   │       └─TableRowIDScan_94         | 6517.14  | 6481    | cop[tikv] | table:rates                                                                            | tikv_task:{pro...| N/A      | N/A  |
|   └─TableReader_84(Probe)             | 984.56   | 1998    | root      |                                                                                        | time:207.6ms, ...| N/A      | N/A  |
|     └─Selection_83                    | 984.56   | 1998    | cop[tikv] |                                                                                        | tikv_task:{pro...| N/A      | N/A  |
|       └─TableRangeScan_82             | 1000.00  | 2048    | cop[tikv] | table:orders                                                                           | tikv_task:{pro...| N/A      | N/A  |
+---------------------------------------+----------+---------+-----------+----------------------------------------------------------------------------------------+---------------...+----------+------+
```

More Detail for understanding execution plans : [TiDB Query Execution Plan Overview](/explain-overview.md) and [EXPLAIN Walkthrough](/explain-walkthrough.md).

## Index strategy in TiDB

TiDB is a distributed SQL database that completely decouples the SQL layer (TiDB Server) from the storage layer (TiKV). Unlike traditional databases, TiDB does not have a buffer pool to cache data at the compute node. As a result, the performance of SQL queries and the TiDB cluster is closely tied to the number of key-value (KV) RPC requests that need to be processed. Typical KV RPC are `Point_Get`, `Batch_Point_Get`, and Coprocessor.

In TiDB, leveraging indexes effectively is crucial for performance tuning, as it can significantly reduce the number of KV RPC requests. By minimizing these requests, you can greatly improve query performance and overall system efficiency. Here are some key strategies:

- Avoiding full table scans
- Avoiding sorting
- Skipping row lookups when possible by using covering index or not request non-needed columns

This section explains the general index strategy and the cost of indexing. Then it showcases three practical examples that illustrate effective indexing strategies in TiDB, focusing on the use of composite and covering indexes for sql tuning.

### Composite index strategy guidelines

When creating a composite index, it's essential to follow a specific order for the columns to ensure optimal performance. This order is crucial because it directly impacts how efficiently the index can filter and sort data.

Here is the recommended order for the columns in the index:

1. Equal predicates for index prefix (columns accessed directly):
   - Equal conditions 
   - `IS NULL` conditions

2. Columns after index prefix for sorting:
   - Leverage index to preprocess sorting
   - Ensure sort and limit pushdown to TiKV
   - Maintain sorted order

3. Additional filtering columns, to reduce row lookups:
   - Time range conditions on datetime columns
   - other non-equal predicates (`!=`, `<>`, `IS NOT NULL`, etc)

4. Include columns in the SELECT list or aggregation to fully utilize a covering index.

### The cost of indexing

While indexes can significantly improve query performance, they also come with costs that should be carefully considered:

1. Performance Impact on Writes:
   - Non-clustered index decreases the chance of single phase commit optimization.
   - Each additional index slows down write operations (INSERT, UPDATE, DELETE)
   - When data is modified, all affected indexes must be updated
   - This overhead increases with each additional index

2. Resource Consumption:
   - Indexes require additional disk space
   - More memory is needed to cache frequently accessed indexes
   - Backup and recovery operations take longer

3. Write Hotspot Risk:
   - Secondary indexes can create write hotspots, for example, a datetime monotonically increasing index will cause the hotspots on the write of the table
   - Performance can degrade significantly if hotspots occur

Best Practice:

- Only create indexes that provide clear performance benefits
- Regularly review index usage statistics via [TIDB_INDEX_USAGE](/information-schema/information-schema-tidb-index-usage.md)
- Consider the write/read ratio of your workload when designing indexes


### SQL tuning with a covering index

A covering index is designed to include all columns referenced in the filter and select clauses. The query below requires an index lookup of 2597411 rows, taking 46.4 seconds to execute. TiDB needs to dispatch 67 cop tasks for the index range scan on logs_idx, identified as `IndexRangeScan_11`, and 301 cop tasks for table access via `TableRowIDScan_12`. By utilizing a covering index, the index lookup can be avoided, leading to improved performance.

Here is the query statement:

```sql
SELECT
  SUM(`logs`.`amount`)
FROM
  `logs`
WHERE
  `logs`.`user_id` = 1111
  AND `logs`.`snapshot_id` IS NULL
  AND `logs`.`status` IN ('complete', 'failure')
  AND `logs`.`source_type` != 'online'
  AND (
    `logs`.`source_type` IN ('user', 'payment')
    OR `logs`.`source_type` IN (
      'bank_account',
    )
    AND `logs`.`target_type` IN ('bank_account')
  );
```

Original plan:

```
+-------------------------------+------------+---------+-----------+--------------------------------------------------------------------------+------------------------------------------------------------+
| id                            | estRows    | actRows | task      | access object                                                            | execution info                                             | 
+-------------------------------+------------+---------+-----------+--------------------------------------------------------------------------+------------------------------------------------------------+
| HashAgg_18                   | 1.00       | 2571625.22 | 1       | root      |                                                              | time:46.4s, loops:2, partial_worker:{wall_time:46.37,   ...|
| └─IndexLookUp_19             | 1.00       | 2570096.68 | 301     | root      |                                                              | time:46.4s, loops:2, index_task: {total_time: 45.8s,    ...|
|   ├─IndexRangeScan_11(Build) | 1309.50    | 317033.98  | 2597411 | cop[tikv] | table:logs, index:logs_idx(snapshot_id, user_id, status)     | time:228ms, loops:2547, cop_task: {num: 67, max: 2.17s, ...|
|   └─HashAgg_7(Probe)         | 1.00       | 588434.48  | 301     | cop[tikv] |                                                              | time:3m46.7s, loops:260, cop_task: {num: 301,           ...|
|     └─Selection_13           | 1271.37    | 561549.27  | 2566562 | cop[tikv] |                                                              | tikv_task:{proc max:10s, min:0s, avg: 915.3ms,          ...|
|       └─TableRowIDScan_12    | 1309.50    | 430861.31  | 2597411 | cop[tikv] | table:logs                                                   | tikv_task:{proc max:10s, min:0s, avg: 908.7ms,          ...|
+-------------------------------+------------+---------+-----------+--------------------------------------------------------------------------+------------------------------------------------------------+
```

After creating the covered index below, which includes the additional columns `source_type`, `target_type`, and `amount`, the query execution time improved to 90ms, and TiDB only needed to send a single cop task to TiKV for data scanning.

Be noted after creating the index, the `ANALYZE TABLE` command is used to gather the statistics for the index. So that the optimizer can leverage the new index immediately. Since In TiDB, index creation will not gather the statistics for the index automatically.

```sql
CREATE INDEX logs_covered ON logs(snapshot_id, user_id, status, source_type, target_type, amount); 
ANALYZE TABLE logs INDEX Logs_covered;
```

New plan:

```
+-------------------------------+------------+---------+-----------+---------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+
| id                            | estRows    | actRows | task      | access object                                                                                                                   | execution info                              |
+-------------------------------+------------+---------+-----------+---------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+
| HashAgg_13                    | 1.00       | 1       | root      |                                                                                                                                 | time:90ms, loops:2, RU:158.885311,       ...|
| └─IndexReader_14              | 1.00       | 1       | root      |                                                                                                                                 | time:89.8ms, loops:2, cop_task: {num: 1, ...|
|   └─HashAgg_6                 | 1.00       | 1       | cop[tikv] |                                                                                                                                 | tikv_task:{time:88ms, loops:52},         ...|
|     └─Selection_12            | 5245632.33 | 52863   | cop[tikv] |                                                                                                                                 | tikv_task:{time:80ms, loops:52}          ...|
|       └─IndexRangeScan_11     | 5245632.33 | 52863   | cop[tikv] | table:logs, index:logs_covered(snapshot_id, user_id, status, source_type, target_type, amount)                                  | tikv_task:{time:60ms, loops:52}          ...|
+-------------------------------+------------+---------+-----------+---------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+
```

### SQL tuning with a composite index involving sorting

When optimizing SQL queries, especially those that include an `ORDER BY` clause, it is beneficial to create a composite index that encompasses both the filtering and sorting columns. This approach allows the database engine to efficiently access the required data while maintaining the desired order.

For instance, consider the following query that retrieves data from `test` based on specific conditions. The execution plan shows a duration of 170ms. TiDB employs the `test_index` to perform an `IndexRangeScan_20` with the filter `snapshot_id = 459840`. Subsequently, it retrieves all columns from the table, resulting in 5715 rows being streamed back to TiDB after the `IndexLookUp_23`, which then sorts the dataset and returns 1000 rows. Note that the `id` column is the primary key, which means it is implicitly included in the `test_idx` index. However, for `IndexRangeScan_20`, the order is not guaranteed because, after the index prefix column `snapshot_id`, there are two additional columns: `user_id` and `status`. Therefore, the ordering of `id` cannot be assured.

Here is the query statement:

```sql
EXPLAIN ANALYZE SELECT  
test.*
FROM
  test
WHERE
  test.snapshot_id = 459840
  AND test.id > 998464
ORDER BY
  test.id ASC
LIMIT
  1000
```

Original plan:

```
+------------------------------+---------+---------+-----------+----------------------------------------------------------+-----------------------------------------------+--------------------------------------------+
| id                           | estRows | actRows | task      | access object                                            | execution info                                | operator info                              | 
+------------------------------+---------+---------+-----------+----------------------------------------------------------+-----------------------------------------------+--------------------------------------------+
| id                           | estRows | actRows | task      | access object                                            | execution info                             ...| test.id, offset:0, count:1000              | 
| TopN_10                      | 19.98   | 1000    | root      |                                                          | time:170.6ms, loops:2                      ...|                                            |
| └─IndexLookUp_23             | 19.98   | 5715    | root      |                                                          | time:166.6ms, loops:7                      ...|                                            | 
|   ├─Selection_22(Build)      | 19.98   | 5715    | cop[tikv] |                                                          | time:18.6ms, loops:9, cop_task: {num: 3,   ...| gt(test.id, 998464)                        | 
|   │ └─IndexRangeScan_20      | 433.47  | 7715    | cop[tikv] | table:test, index:test_idx(snapshot_id, user_id, status) | tikv_task:{proc max:4ms, min:4ms, avg: 4ms ...| range:[459840,459840], keep order:false    |
|   └─TableRowIDScan_21(Probe) | 19.98   | 5715    | cop[tikv] | table:test                                               | time:301.6ms, loops:10, cop_task: {num: 3, ...| keep order:false                           | 
+------------------------------+---------+---------+-----------+----------------------------------------------------------+-----------------------------------------------+--------------------------------------------+
```

To optimize the query, we can create a new index on `(snapshot_id)` to ensure that for each snapshot_id value, the id is sorted in the index. By utilizing this new index, the query execution time is reduced to 96ms. Note that the `keep order` is true for `IndexRangeScan_33`, and the `TopN` is replaced with `Limit`. For the `IndexLookUp_35`, only 1000 rows are returned to TiDB, eliminating the need for additional sorting operations.

Here is the query statement:

```sql
CREATE INDEX test_new ON test(snapshot_id);
ANALYZE TABLE test INDEX test_new;
```

New plan:

```
+----------------------------------+---------+---------+-----------+----------------------------------------------+----------------------------------------------+----------------------------------------------------+
| id                               | estRows | actRows | task      | access object                                | execution info                               | operator info                                      |
+----------------------------------+---------+---------+-----------+----------------------------------------------+----------------------------------------------+----------------------------------------------------+
| Limit_14                         | 17.59   | 1000    | root      |                                              | time:96.1ms, loops:2, RU:92.300155           | offset:0, count:1000                               |
| └─IndexLookUp_35                 | 17.59   | 1000    | root      |                                              | time:96.1ms, loops:1, index_task:         ...|                                                    |
|   ├─IndexRangeScan_33(Build)     | 17.59   | 5715    | cop[tikv] | table:test, index:test_new(snapshot_id)      | time:7.25ms, loops:8, cop_task: {num: 3,  ...| range:(459840 998464,459840 +inf], keep order:true |
|   └─TableRowIDScan_34(Probe)     | 17.59   | 5715    | cop[tikv] | table:test                                   | time:232.9ms, loops:9, cop_task: {num: 3, ...| keep order:false                                   |
+----------------------------------+---------+---------+-----------+----------------------------------------------+----------------------------------------------+----------------------------------------------------+
```

### SQL tuning with composite indexes for efficient filtering and sorting

The original query below took 11 minutes and 9 seconds to complete, which is an extremely long execution time for a query that only needs to return 101 rows. This poor performance can be attributed to several factors:

1. Inefficient index usage: The optimizer chose the index on `created_at`, which resulted in scanning 25,147,450 rows.
2. Large intermediate result set: After applying the date range filter, 12,082,311 rows still needed to be processed.
3. Late filtering: The most selective predicates `(mode, user_id, and label_id)` were applied after accessing the table, resulting in 16,604 rows.
4. Sorting overhead: The final sort operation on 16,604 rows added additional processing time.

Here is the query statement:

```sql
SELECT `orders`.*
FROM `orders`
WHERE 
    `orders`.`mode` = 'production'
    AND `orders`.`user_id` = 11111
    AND orders.label_id IS NOT NULL
    AND orders.created_at >= '2024-04-07 18:07:52'
    AND orders.created_at <= '2024-05-11 18:07:52'
    AND orders.id >= 1000000000
    AND orders.id < 1500000000
ORDER BY orders.id DESC 
LIMIT 101;
```

Here is the index on `orders`

```sql
PRIMARY KEY (`id`),
UNIQUE KEY `index_orders_on_adjustment_id` (`adjustment_id`),
KEY `index_orders_on_user_id` (`user_id`),
KEY `index_orders_on_label_id` (`label_id`),
KEY `index_orders_on_created_at` (`created_at`)
```

Original plan:

```
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------+----------+------+
| id                             | estRows   | actRows | task      | access object                                                                  | execution info                                      | operator info                                                                          | memory   | disk |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------+----------+------+
| TopN_10                        | 101.00    | 101     | root      |                                                                                | time:11m9.8s, loops:2                               | orders.id:desc, offset:0, count:101                                                    | 271 KB   | N/A  |
| └─IndexLookUp_39               | 173.83    | 16604   | root      |                                                                                | time:11m9.8s, loops:19, index_task: {total_time:...}|                                                                                        | 20.4 MB  | N/A  |
|   ├─Selection_37(Build)        | 8296.70   | 12082311| cop[tikv] |                                                                                | time:26.4ms, loops:11834, cop_task: {num: 294, m...}| ge(orders.id, 1000000000), lt(orders.id, 1500000000)                                   | N/A      | N/A  |
|   │ └─IndexRangeScan_35        | 6934161.90| 25147450| cop[tikv] | table:orders, index:index_orders_on_created_at(created_at)                     | tikv_task:{proc max:2.15s, min:0s, avg: 58.9ms, ...}| range:[2024-04-07 18:07:52,2024-05-11 18:07:52), keep order:false                      | N/A      | N/A  |
|   └─Selection_38(Probe)        | 173.83    | 16604   | cop[tikv] |                                                                                | time:54m46.2s, loops:651, cop_task: {num: 1076, ...}| eq(orders.mode, "production"), eq(orders.user_id, 11111), not(isnull(orders.label_id)) | N/A      | N/A  |
|     └─TableRowIDScan_36        | 8296.70   | 12082311| cop[tikv] | table:orders                                                                   | tikv_task:{proc max:44.8s, min:0s, avg: 3.33s, p...}| keep order:false                                                                       | N/A      | N/A  |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------+----------+------+
```

After creating the new index `idx_composite` on `orders(user_id, mode, id, created_at, label_id)`, the query performance improved dramatically. The execution time reduced from 11 minutes and 9 seconds to just 5.3 milliseconds, which is a staggering improvement of over 126,000 times faster. This massive improvement can be explained by:

1. Efficient index usage: The new index allows for an index range scan on user_id, mode, and id, which are the most selective predicates. This drastically reduces the number of rows scanned from millions to just 224.
2. Index-only sort: The `keep order:true` in the plan indicates that the index structure is used for sorting, avoiding a separate sort operation.
3. Early filtering: The most selective predicates are applied first, quickly reducing the result set to 224 rows before further filtering.
4. Limit push-down: The LIMIT clause is pushed down to the index scan, allowing early termination of the scan once 101 rows are found.

This case demonstrates the profound impact that a well-designed index can have on query performance. By aligning the index structure with the query's predicates, sort order, and required columns, we achieved a performance improvement of over five orders of magnitude.

```sql
CREATE INDEX idx_composite ON Orders(user_id, mode, id, created_at, label_id);
ANALYZE TABLE orders index idx_composite;
```

New plan:

```
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+
| id                             | estRows   | actRows | task      | access object                                                                  | execution info                                      | operator info                                                                                                        | memory   | disk |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+
| IndexLookUp_32                 | 101.00    | 101     | root      |                                                                                | time:5.3ms, loops:2, RU:3.435006, index_task: {t...}| limit embedded(offset:0, count:101)                                                                                  | 128.5 KB | N/A  |
| ├─Limit_31(Build)              | 101.00    | 101     | cop[tikv] |                                                                                | time:1.35ms, loops:1, cop_task: {num: 1, max: 1....}| offset:0, count:101                                                                                                  | N/A      | N/A  |
| │ └─Selection_30               | 535.77    | 224     | cop[tikv] |                                                                                | tikv_task:{time:0s, loops:3}                        | ge(orders.created_at, 2024-04-07 18:07:52), le(orders.created_at, 2024-05-11 18:07:52), not(isnull(orders.label_id)) | N/A      | N/A  |
| │   └─IndexRangeScan_28        | 503893.42 | 224     | cop[tikv] | table:orders, index:idx_composite(user_id, mode, id, created_at, label_id)     | tikv_task:{time:0s, loops:3}                        | range:[11111 "production" 1000000000,11111 "production" 1500000000), keep order:true, desc                           | N/A      | N/A  |
| └─TableRowIDScan_29(Probe)     | 101.00    | 101     | cop[tikv] | table:orders                                                                   | time:2.9ms, loops:2, cop_task: {num: 3, max: 2.7...}| keep order:false                                                                                                     | N/A      | N/A  |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+

```

## When to use TiFlash: a simple guide

This section explains the best scenarios for using TiFlash in TiDB. TiFlash is optimized for analytical queries that involve complex calculations, aggregations, and large dataset scans, thanks to its columnar storage format. Here’s when to consider TiFlash:

- Large-Scale Data Analysis: For queries involving extensive data scans, such as OLAP workloads, TiFlash can deliver faster performance compared to TiKV due to its columnar storage and MPP mode.
- Complex Scans, Aggregations and Joins: TiFlash is ideal for queries with heavy aggregations and joins, as it can process these operations more efficiently by only reading necessary columns.
- Mixed Workloads: In hybrid environments where both transactional (OLTP) and analytical (OLAP) workloads run simultaneously, TiFlash can handle analytical queries without impacting TiKV’s performance for transactional queries.
- SaaS Arbitrary Filtering Workloads: In SaaS workloads, there is often a need for arbitrary filtering across many columns. Indexing all columns is impractical, especially when the primary key includes a tenant ID and all queries include this ID. TiFlash is an ideal solution here: because data in TiFlash is sorted and clustered by primary key, and with the [late materialization](/tiflash/tiflash-late-materialization.md) feature, it enables efficient table range scans. This allows TiFlash to provide fast performance without the overhead of maintaining multiple indexes.

Using TiFlash strategically can enhance query performance and optimize resource usage in TiDB for data-intensive, analytical queries. Here are two use case of TiFlash

### Analytical query

Take the TPC-H Query 14 as an example. The original query requires joining the order_line and item tables. The plan for executing this query on the TiKV storage engine takes 21.1 seconds. The plan for executing the same query using TiFlash MPP mode takes only 1.41 seconds, which is 15 times faster than the TiKV plan.

- TiKV plan: TiDB needs to fetch 3,864,397 rows from the lineitem table and 10 million rows from the part table. The hash join operation (`HashJoin_21`) is performed at the TiDB, along with the subsequent projection (`Projection_38`) and aggregation (`HashAgg_9`) operations.
- TiFlash plan: The optimizer recognizes that both the order_line and item tables have TiFlash replicas available. TiDB automatically selects the MPP mode based on the optimizer's cost estimation. Under the MPP mode, the entire query is executed on TiFlash columnar storage engine, including the table scans, hash join, column projection and aggregation, leads to the significant performance improvement compared to the TiKV plan. 
 
Here is the query statement:

```sql
select 100.00 * sum(case when i_data like 'PR%' then ol_amount else 0 end) / (1+sum(ol_amount)) as promo_revenue
from	order_line, item
where	ol_i_id = i_id and ol_delivery_d >= '2007-01-02 00:00:00.000000' and ol_delivery_d < '2030-01-02 00:00:00.000000';
```

TiKV plan

```
+-------------------------------+--------------+-----------+-----------+----------------+----------------------------------------------+
| ID                            | ESTROWS      | ACTROWS   | TASK      | ACCESS OBJECT  | EXECUTION INFO                               |
+-------------------------------+--------------+-----------+-----------+----------------+----------------------------------------------+
| Projection_8                  | 1.00         | 1         | root      |                | time:21.1s, loops:2, RU:1023225.707561, ...  |
| └─HashAgg_9                   | 1.00         | 1         | root      |                | time:21.1s, loops:2, partial_worker:{ ...    |
|   └─Projection_38             | 3839984.46   | 3864397   | root      |                | time:21.1s, loops:3776, Concurrency:5        |
|     └─HashJoin_21             | 3839984.46   | 3864397   | root      |                | time:21.1s, loops:3776, build_hash_table:... |
|       ├─TableReader_24(Build) | 3826762.62   | 3864397   | root      |                | time:18.4s, loops:3764, cop_task: ...        |
|       │ └─Selection_23        | 3826762.62   | 3864397   | cop[tikv] |                | tikv_task:{proc max:717ms, min:265ms, ...    |
|       │   └─TableFullScan_22  | 300005811.00 | 300005811 | cop[tikv] | table:lineitem | tikv_task:{proc max:685ms, min:252ms, ...    |
|       └─TableReader_26(Probe) | 10000000.00  | 10000000  | root      |                | time:1.29s, loops:9780, cop_task: ...        |
|         └─TableFullScan_25    | 10000000.00  | 10000000  | cop[tikv] | table:part     | tikv_task:{proc max:922ms, min:468ms, ...    |
+-------------------------------+--------------+-----------+-----------+----------------+----------------------------------------------+
```

TiFlash plan

```
+--------------------------------------------+-------------+----------+--------------+----------------+--------------------------------------+
| ID                                         | ESTROWS     | ACTROWS  | TASK         | ACCESS OBJECT  | EXECUTION INFO                       |
+--------------------------------------------+-------------+----------+--------------+----------------+--------------------------------------+
| Projection_8                               | 1.00        | 1        | root         |                | time:1.41s, loops:2, RU:45879.127909 |
| └─HashAgg_52                               | 1.00        | 1        | root         |                | time:1.41s, loops:2, ...             |
|   └─TableReader_54                         | 1.00        | 1        | root         |                | time:1.41s, loops:2, ...             |
|     └─ExchangeSender_53                    | 1.00        | 1        | mpp[tiflash] |                | tiflash_task:{time:1.41s, ...        |
|       └─HashAgg_13                         | 1.00        | 1        | mpp[tiflash] |                | tiflash_task:{time:1.41s, ...        |
|         └─Projection_74                    | 3813443.11  | 3864397  | mpp[tiflash] |                | tiflash_task:{time:1.4s, ...         |
|           └─Projection_51                  | 3813443.11  | 3864397  | mpp[tiflash] |                | tiflash_task:{time:1.39s, ...        |
|             └─HashJoin_50                  | 3813443.11  | 3864397  | mpp[tiflash] |                | tiflash_task:{time:1.39s, ...        |
|               ├─ExchangeReceiver_31(Build) | 3800312.67  | 3864397  | mpp[tiflash] |                | tiflash_task:{time:1.05s, ...        |
|               │ └─ExchangeSender_30        | 3800312.67  | 3864397  | mpp[tiflash] |                | tiflash_task:{time:1.2s, ...         |
|               │   └─TableFullScan_28       | 3800312.67  | 3864397  | mpp[tiflash] | table:lineitem | tiflash_task:{time:1.15s, ...        |
|               └─ExchangeReceiver_34(Probe) | 10000000.00 | 10000000 | mpp[tiflash] |                | tiflash_task:{time:1.24s, ...        |
|                 └─ExchangeSender_33        | 10000000.00 | 10000000 | mpp[tiflash] |                | tiflash_task:{time:1.4s, ...         |
|                   └─TableFullScan_32       | 10000000.00 | 10000000 | mpp[tiflash] | table:part     | tiflash_task:{time:59.2ms, ...       |
+--------------------------------------------+-------------+----------+--------------+----------------+--------------------------------------+
```

### SaaS arbitrary filtering workloads

#### Overview

In SaaS applications, it's common to structure tables with composite primary keys that include tenant identification. Let's look at a typical example where TiFlash can significantly improve query performance.

#### Case study: multi-tenant data access

Consider a table design with a composite primary key: `(tenantId, objectTypeId, objectId)`. A common query pattern is to:

- Fetch the first N records for a specific tenant and object type, and applying random filters across hundreds or thousands of other columns, Making it impractical to create indexes for every possible filter combination. There is a potential sort operator after the filter as well.
- Get the total count of records matching these criterias

#### Performance comparison

- TiKV Plan: When executing this query on TiKV storage engine, it takes 2 minutes 38.6 seconds. `TableRangeScan` need to send 5121 cop tasks since data is spread over 5121 regions.
- TiFlash Plan: The same query on TiFlash MPP engine takes only 3.44 seconds - almost 46 times faster. Since data in TiFlash is sorted by primary key, queries filtered by the primary key's prefix will also use a `TableRangeScan` instead of a full table scan. Compared to TiKV, TiFlash only uses 2 mpp tasks.

Here is the query statement:

```sql
WITH `results` AS (
  SELECT field1, field2, field3, field4
  FROM usertable
  where tenantId = 1234 and objectTypeId = 6789
),
`limited_results` AS (
  SELECT field1, field2, field3, field4
  FROM `results` LIMIT 100
)
SELECT field1, field2, field3, field4
FROM
  (
    SELECT 100 `__total__`, field1, field2, field3, field4
    FROM `limited_results`
    UNION ALL
    SELECT count(*) `__total__`, field1, field2, field3, field4
    FROM `results`
  ) `result_and_count`;
```

TiKV plan

```
+--------------------------------+-----------+---------+-----------+-----------------------+-----------------------------------------------------+
| id                             | estRows   | actRows | task      | access object         | execution info                                      |
+--------------------------------+-----------+---------+-----------+-----------------------+-----------------------------------------------------+
| Union_18                       | 101.00    | 101     | root      |                       | time:2m38.6s, loops:3, RU:8662189.451027            |
| ├─Limit_20                     | 100.00    | 100     | root      |                       | time:23ms, loops:2                                  |
| │ └─TableReader_25             | 100.00    | 100     | root      |                       | time:23ms, loops:1, cop_task: {num: 1, max: 22.8...}|
| │   └─Limit_24                 | 100.00    | 100     | cop[tikv] |                       | tikv_task:{time:21ms, loops:3}, scan_detail: {...}  |
| │     └─TableRangeScan_22      | 100.00    | 100     | cop[tikv] | table:usertable       | tikv_task:{time:21ms, loops:3}                      |
| └─Projection_26                | 1.00      | 1       | root      |                       | time:2m38.6s, loops:2, Concurrency:OFF              |
|   └─HashAgg_34                 | 1.00      | 1       | root      |                       | time:2m38.6s, loops:2, partial_worker:{...}, fin.. .|
|     └─TableReader_35           | 1.00      | 5121    | root      |                       | time:2m38.6s, loops:7, cop_task: {num: 5121, max:...|
|       └─HashAgg_27             | 1.00      | 5121    | cop[tikv] |                       | tikv_task:{proc max:0s, min:0s, avg: 462.8ms, p...} |
|         └─TableRangeScan_32    | 10000000  | 10000000| cop[tikv] | table:usertable       | tikv_task:{proc max:0s, min:0s, avg: 460.5ms, p...} |
+--------------------------------+-----------+---------+-----------+-----------------------+-----------------------------------------------------+
```

TiFlash plan

```
+--------------------------------+-----------+---------+--------------+--------------------+-----------------------------------------------------+
| id                             | estRows   | actRows | task         | access object      | execution info                                      |
+--------------------------------+-----------+---------+--------------+--------------------+-----------------------------------------------------+
| Union_18                       | 101.00    | 101     | root         |                    | time:3.44s, loops:3, RU:0.000000                    |
| ├─Limit_22                     | 100.00    | 100     | root         |                    | time:146.7ms, loops:2                               |
| │ └─TableReader_30             | 100.00    | 100     | root         |                    | time:146.7ms, loops:1, cop_task: {num: 1, max: 0...}|
| │   └─ExchangeSender_29        | 100.00    | 0       | mpp[tiflash] |                    |                                                     |
| │     └─Limit_28               | 100.00    | 0       | mpp[tiflash] |                    |                                                     |
| │       └─TableRangeScan_27    | 100.00    | 0       | mpp[tiflash] | table:usertable    |                                                     |
| └─Projection_31                | 1.00      | 1       | root         |                    | time:3.42s, loops:2, Concurrency:OFF                |
|   └─HashAgg_49                 | 1.00      | 1       | root         |                    | time:3.42s, loops:2, partial_worker:{...}, fin...   |
|     └─TableReader_51           | 1.00      | 2       | root         |                    | time:3.42s, loops:2, cop_task: {num: 4, max: 0...}  |
|       └─ExchangeSender_50      | 1.00      | 2       | mpp[tiflash] |                    | tiflash_task:{proc max:3.4s, min:3.15s, avg: 3...}  |
|         └─HashAgg_36           | 1.00      | 2       | mpp[tiflash] |                    | tiflash_task:{proc max:3.4s, min:3.15s, avg: 3...}  |
|           └─TableRangeScan_48 | 10000000   | 10000000| mpp[tiflash] | table:usertable    | tiflash_task:{proc max:3.4s, min:3.15s, avg: 3...}  |
+--------------------------------+-----------+---------+--------------+--------------------+-----------------------------------------------------+
```

#### Query routing between TiKV and TiFlash

After enabling TiFlash replicas for tables with large amounts of multi-tenant data, the optimizer can make choices to serve the query on TiKV or TiFlash storage engine based on the row count:

- Small Tenants: TiKV is preferred for tenants with small data size, as it provides high concurrency for small queries with table range scan.
- Large Tenants: For tenants with large datasets (like 10M rows in this case), TiFlash is more efficient as it:
    - Handles dynamic filtering conditions without requiring specific indexes
    - Pushes down count, sort and limit operations to TiFlash
    - Leverages columnar storage to scan only the required columns