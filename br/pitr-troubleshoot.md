---
title: Troubleshoot PITR Log Backup
summary: Learn about common failures of log backup and solutions of the PITR feature.
---

# Troubleshoot PITR Log Backup

This document summarizes some common issues when performing PITR log backup and the solutions.

## What should I do if the data cannot be queried from the TiFlash engine when using the `br restore point` command to restore the downstream cluster?

In v6.2.0, it is not recommended to restore the downstream TiFlash replicas when using PITR. After restoring data, you need to execute the following statement to set the TiFlash replica of the schema or table.

```sql
ALTER TABLE table_name SET TIFLASH REPLICA count;
```

## What should I do if the `status` of a log backup task becomes `ERROR`?

In the process of performing a log backup, the task status becomes `ERROR` if it cannot be recovered after retrying. The following is an example:

```shell
br log status --pd x.x.x.x:2379

● Total 1 Tasks.
> #1 <
                    name: task1
                  status: ○ ERROR
                   start: 2022-07-25 13:49:02.868 +0000
                     end: 2090-11-18 14:07:45.624 +0000
                 storage: s3://tmp/br-log-backup0ef49055-5198-4be3-beab-d382a2189efb/Log
             speed(est.): 0.00 ops/s
      checkpoint[global]: 2022-07-25 14:46:50.118 +0000; gap=11h31m29s
          error[store=1]: KV:LogBackup:RaftReq
error-happen-at[store=1]: 2022-07-25 14:54:44.467 +0000; gap=11h23m35s
  error-message[store=1]: retry time exceeds: and error failed to get initial snapshot: failed to get the snapshot (region_id = 94812): Error during requesting raftstore: message: "read index not ready, reason can not read index due to merge, region 94812" read_index_not_ready { reason: "can not read index due to merge" region_id: 94812 }: failed to get initial snapshot: failed to get the snapshot (region_id = 94812): Error during requesting raftstore: message: "read index not ready, reason can not read index due to merge, region 94812" read_index_not_ready { reason: "can not read index due to merge" region_id: 94812 }: failed to get initial snapshot: failed to get the snapshot (region_id = 94812): Error during requesting raftstore: message: "read index not ready, reason can not read index due to merge, region 94812" read_index_not_ready { reason: "can not read index due to merge" region_id: 94812 }
```

At this time, you need to confirm the cause of the failure and resolve it according to the error message. To restore the backup task after resolving the failure, you can use the following command:

```shell
br log resume --task-name=task1 --pd x.x.x.x:2379
```

After the backup task is restored, you can check the status using `br log status` again. The backup task continues when the task status becomes `NORMAL`.

```shell
● Total 1 Tasks.
> #1 <
              name: task1
            status: ● NORMAL
             start: 2022-07-25 13:49:02.868 +0000
               end: 2090-11-18 14:07:45.624 +0000
           storage: s3://tmp/br-log-backup0ef49055-5198-4be3-beab-d382a2189efb/Log
       speed(est.): 15509.75 ops/s
checkpoint[global]: 2022-07-25 14:46:50.118 +0000; gap=6m28s
```

> **Note:**
>
> Since the feature backs up multiple versions of data, when the task fails and the status becomes `ERROR`, the checkpoint data of the current task is set as a `safe point`, and the data of the `safe point` will not be garbage collected within 24 hours. So, when the task is restored, the backup task continues from the last checkpoint. If the task fails for more than 24 hours, the last checkpoint data has been garbage collected, and you can only execute the `br log stop` command to stop the task first and then start a new backup task.

## What should I do if the error message `ErrBackupGCSafepointExceeded` is returned when executing the `br log resume` command to resume the suspended task?

```shell
Error: failed to check gc safePoint, checkpoint ts 433177834291200000: GC safepoint 433193092308795392 exceed TS 433177834291200000: [BR:Backup:ErrBackupGCSafepointExceeded]backup GC safepoint exceeded
```

After pausing the log backup task, the pausing task program set the current checkpoint as service safepoint automatically to prevent the MVCC data generated by the changelog from being garbage collected. The MVCC data can be saved in 24 hours. When more than 24 hours, the MVCC data of the checkpoint has been garbage collected, and the backup task is unable to resume.

The solution to the preceding scenario is:

1. Delete the current task using `br log stop`.
2. Create a new log backup task using `br log start` and make a full backup at the same time for subsequent PITR recovery operations.

## What should I do if an error occurs when executing the Exchange Partition DDL during PITR recovery?

If the following error occurs when executing the PITR recover log:

```
restore of ddl `exchange-table-partition` is not supported
```

Since the log backup feature of v6.2.0 is not compatible with Exchange Partition DDL, it is recommended to avoid using exchange partition DDL during log backup. If you have executed the DDL, you must make a full backup immediately, and then PITR can restore the log data after the full backup checkpoint.
