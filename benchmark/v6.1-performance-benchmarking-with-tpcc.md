---
title: TiDB TPC-C Performance Test Report -- v6.1.0 vs. v6.0.0
---

# TiDB TPC-C パフォーマンス テスト レポート -- v6.1.0 と v6.0.0 {#tidb-tpc-c-performance-test-report-v6-1-0-vs-v6-0-0}

## テストの概要 {#test-overview}

このテストは、オンライン トランザクション処理 (OLTP) シナリオにおける TiDB v6.1.0 と v6.0.0 の TPC-C パフォーマンスを比較することを目的としています。結果は、v6.0.0 と比較して、v6.1.0 の TPC-C パフォーマンスが 2.85% 向上していることを示しています。

## テスト環境（AWS EC2） {#test-environment-aws-ec2}

### ハードウェア構成 {#hardware-configuration}

| サービスの種類 | EC2タイプ     | インスタンス数 |
| :------ | :--------- | :------ |
| PD      | m5.xlarge  | 3       |
| TiKV    | i3.4xlarge | 3       |
| TiDB    | c5.4xlarge | 3       |
| TPC-C   | c5.9xlarge | 1       |

### ソフトウェアバージョン {#software-version}

| サービスの種類 | ソフトウェアバージョン       |
| :------ | :---------------- |
| PD      | v6.0.0 および v6.1.0 |
| TiDB    | v6.0.0 および v6.1.0 |
| TiKV    | v6.0.0 および v6.1.0 |
| TiUP    | 1.9.3             |
| HAプロキシ  | 2.5.0             |

### パラメータ設定 {#parameter-configuration}

TiDB v6.1.0 と TiDB v6.0.0 は同じ構成を使用します。

#### TiDBパラメータの設定 {#tidb-parameter-configuration}

```yaml
log.level: "error"
prepared-plan-cache.enabled: true
tikv-client.max-batch-wait-time: 2000000
```

#### TiKVパラメータ設定 {#tikv-parameter-configuration}

```yaml
raftstore.apply-max-batch-size: 2048
raftstore.apply-pool-size: 3
raftstore.store-max-batch-size: 2048
raftstore.store-pool-size: 2
readpool.storage.normal-concurrency: 10
server.grpc-concurrency: 6
```

#### TiDB グローバル変数の設定 {#tidb-global-variable-configuration}

```sql
set global tidb_hashagg_final_concurrency=1;
set global tidb_hashagg_partial_concurrency=1;
set global tidb_enable_async_commit = 1;
set global tidb_enable_1pc = 1;
set global tidb_guarantee_linearizability = 0;
set global tidb_enable_clustered_index = 1;
set global tidb_prepared_plan_cache_size=1000;
```

#### HAProxy 構成 - haproxy.cfg {#haproxy-configuration-haproxy-cfg}

TiDB で HAProxy を使用する方法の詳細については、 [TiDB で HAProxy を使用するためのベスト プラクティス](/best-practices/haproxy-best-practices.md)を参照してください。

```yaml
global                                     # Global configuration.
   pidfile     /var/run/haproxy.pid        # Writes the PIDs of HAProxy processes into this file.
   maxconn     4000                        # The maximum number of concurrent connections for a single HAProxy process.
   user        haproxy                     # The same with the UID parameter.
   group       haproxy                     # The same with the GID parameter. A dedicated user group is recommended.
   nbproc      64                          # The number of processes created when going daemon. When starting multiple processes to forward requests, ensure that the value is large enough so that HAProxy does not block processes.
   daemon                                  # Makes the process fork into background. It is equivalent to the command line "-D" argument. It can be disabled by the command line "-db" argument.

defaults                                   # Default configuration.
   log global                              # Inherits the settings of the global configuration.
   retries 2                               # The maximum number of retries to connect to an upstream server. If the number of connection attempts exceeds the value, the backend server is considered unavailable.
   timeout connect  2s                     # The maximum time to wait for a connection attempt to a backend server to succeed. It should be set to a shorter time if the server is located on the same LAN as HAProxy.
   timeout client 30000s                   # The maximum inactivity time on the client side.
   timeout server 30000s                   # The maximum inactivity time on the server side.

listen tidb-cluster                        # Database load balancing.
   bind 0.0.0.0:3390                       # The Floating IP address and listening port.
   mode tcp                                # HAProxy uses layer 4, the transport layer.
   balance leastconn                      # The server with the fewest connections receives the connection. "leastconn" is recommended where long sessions are expected, such as LDAP, SQL and TSE, rather than protocols using short sessions, such as HTTP. The algorithm is dynamic, which means that server weights might be adjusted on the fly for slow starts for instance.
   server tidb-1 10.9.18.229:4000 check inter 2000 rise 2 fall 3       # Detects port 4000 at a frequency of once every 2000 milliseconds. If it is detected as successful twice, the server is considered available; if it is detected as failed three times, the server is considered unavailable.
   server tidb-2 10.9.39.208:4000 check inter 2000 rise 2 fall 3
   server tidb-3 10.9.64.166:4000 check inter 2000 rise 2 fall 3
```

### テストデータの準備 {#prepare-test-data}

1.  TiUPを使用して TiDB v6.1.0 および v6.0.0をデプロイ。
2.  `tpcc` : `create database tpcc;`という名前のデータベースを作成します。
3.  BenchmarkSQL を使用して、TPC-C `tiup bench tpcc prepare --warehouse 5000 --db tpcc -H 127.0.0.1 -p 4000` Warehouse データをインポートします。
4.  `tiup bench tpcc run -U root --db tpcc --host 127.0.0.1 --port 4000 --time 1800s --warehouses 5000 --threads {{thread}}`コマンドを実行して、HAProxy 経由で TiDB でストレス テストを実行します。同時実行ごとにテストには 30 分かかります。
5.  結果からNew OrderのtpmCデータを抽出します。

## テスト結果 {#test-result}

v6.0.0 と比較して、v6.1.0 の TPC-C パフォーマンスは**2.85% 向上してい**ます。

| スレッド | v6.0.0 tpmC | v6.1.0 tpmC | tpmC 改善 (%) |
| :--- | :---------- | :---------- | :---------- |
| 50   | 59059.2     | 60424.4     | 2.31        |
| 100  | 69357.6     | 71235.5     | 2.71        |
| 200  | 71364.8     | 74117.8     | 3.86        |
| 400  | 72694.3     | 74525.3     | 2.52        |

![TPC-C](/media/tpcc_v600_vs_v610.png)
